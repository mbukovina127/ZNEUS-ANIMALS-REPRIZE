{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning - Image Classification",
   "id": "de369611418338b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:51:03.707007Z",
     "start_time": "2025-11-28T15:51:03.702663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#importing\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fun\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *"
   ],
   "id": "3d39034e8143cb63",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Database importing",
   "id": "36370301af86524b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:51:47.724420Z",
     "start_time": "2025-11-28T15:51:03.717696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: pouzi toto martin\n",
    "load_dotenv()\n",
    "DAT_PATH = os.getenv(\"TRAIN_DATASET_PATH\")\n",
    "\n",
    "ANIMALS_DATAFRAME = load_dataset_info(\"../data/archive/raw-img\")"
   ],
   "id": "d9710a1ff3f0f4bf",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:51:47.773148Z",
     "start_time": "2025-11-28T15:51:47.757734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df, test_df = train_test_split(ANIMALS_DATAFRAME.sample(frac=0.5, random_state=37), test_size=0.01, random_state=37)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.0526, random_state=37)"
   ],
   "id": "5cc1f9d00df31223",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:54:29.607976Z",
     "start_time": "2025-11-28T15:54:29.596545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#setting up dataloaders\n",
    "\n",
    "TARGET_SIZE = (256, 256)\n",
    "MAX_SIZE = 500\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = ANIMALS_DATAFRAME.label.nunique()\n",
    "CLASS_LABELS = {name: idx for idx, name in enumerate(np.sort(ANIMALS_DATAFRAME.label.unique()))}\n",
    "\n",
    "# vypocet frekvencie augmentacie -> aby nebol model biasnuty iba na majoritne categorie obrazkov\n",
    "counts = train_df.label.value_counts()\n",
    "max_count = counts.max()\n",
    "aug_strength = (max_count / counts).to_dict()\n",
    "\n",
    "print(aug_strength)\n",
    "\n",
    "train_gen = AnimalImageGenerator(\n",
    "    df=train_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    augment=True,\n",
    "    shuffle=True,\n",
    "    aug_strength=aug_strength,\n",
    "    max_size=MAX_SIZE,\n",
    "    class_mapping=CLASS_LABELS,\n",
    ")\n",
    "\n",
    "test_gen = AnimalImageGenerator(\n",
    "    df=test_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    augment=False,\n",
    "    shuffle=False,\n",
    "    max_size=MAX_SIZE,\n",
    "    class_mapping=CLASS_LABELS,\n",
    ")\n",
    "\n",
    "val_gen = AnimalImageGenerator(\n",
    "    df=val_df,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    target_size=TARGET_SIZE,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    augment=False,\n",
    "    shuffle=False,\n",
    "    max_size=MAX_SIZE,\n",
    "    class_mapping=CLASS_LABELS,\n",
    ")\n"
   ],
   "id": "6a18fe6f83c8e0b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cane': 1.0, 'ragno': 1.0594104308390022, 'gallina': 1.5934515688949522, 'cavallo': 1.9194741166803615, 'farfalla': 2.3524672708962737, 'mucca': 2.636568848758465, 'scoiattolo': 2.6485260770975056, 'pecora': 2.7257876312718787, 'gatto': 3.0817941952506596, 'elefante': 3.4505169867060563}\n",
      "---\n",
      "Total images: 12277\n",
      "Num classes: 10\n",
      "---\n",
      "---\n",
      "Total images: 131\n",
      "Num classes: 10\n",
      "---\n",
      "---\n",
      "Total images: 682\n",
      "Num classes: 10\n",
      "---\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4945c56cb8443cdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:55:00.571270Z",
     "start_time": "2025-11-28T15:54:32.898722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#testovaci vypis\n",
    "\n",
    "images, labels = train_gen[0]     # zavolá __getitem__(0) pre prvy batch\n",
    "print(\"Images shape:\", images.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "print(\"Image dtype:\", images.dtype)\n",
    "print(\"Label sample:\", labels[0])"
   ],
   "id": "d2b55cb221211796",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/95 [00:00<01:08,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 256, 256, 3) (202,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/95 [00:01<01:08,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206, 256, 256, 3) (206,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/95 [00:02<01:15,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 256, 256, 3) (208,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/95 [00:03<01:09,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 256, 256, 3) (196,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/95 [00:03<01:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 256, 256, 3) (196,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6/95 [00:04<01:07,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 256, 256, 3) (199,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/95 [00:05<01:03,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193, 256, 256, 3) (193,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/95 [00:05<01:01,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 256, 256, 3) (204,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/95 [00:06<01:03,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218, 256, 256, 3) (218,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/95 [00:07<01:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 256, 256, 3) (196,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11/95 [00:08<00:59,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 256, 256, 3) (204,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12/95 [00:09<01:03,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 256, 256, 3) (174,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 13/95 [00:09<01:03,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 256, 256, 3) (190,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 14/95 [00:10<00:59,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 256, 256, 3) (189,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 15/95 [00:11<01:01,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 256, 256, 3) (186,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 16/95 [00:12<00:58,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 256, 256, 3) (198,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 17/95 [00:12<00:55,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 256, 256, 3) (192,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 18/95 [00:13<00:55,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 256, 256, 3) (195,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 19/95 [00:14<00:55,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 256, 256, 3) (199,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 20/95 [00:15<01:02,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 256, 256, 3) (196,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 21/95 [00:15<01:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 256, 256, 3) (221,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 22/95 [00:16<00:58,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212, 256, 256, 3) (212,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 23/95 [00:17<00:53,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177, 256, 256, 3) (177,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 24/95 [00:18<00:51,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 256, 256, 3) (190,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 25/95 [00:18<00:49,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 256, 256, 3) (186,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 26/95 [00:19<00:49,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 256, 256, 3) (200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 27/95 [00:20<00:50,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 256, 256, 3) (204,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 28/95 [00:20<00:49,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 256, 256, 3) (203,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 29/95 [00:21<00:47,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(186, 256, 256, 3) (186,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 30/95 [00:22<00:45,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(189, 256, 256, 3) (189,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 31/95 [00:23<00:45,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197, 256, 256, 3) (197,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 32/95 [00:23<00:44,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(198, 256, 256, 3) (198,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 33/95 [00:24<00:43,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 256, 256, 3) (202,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 34/95 [00:25<00:50,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215, 256, 256, 3) (215,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 35/95 [00:26<00:46,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 256, 256, 3) (183,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 35/95 [00:26<00:45,  1.32it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#testovaci vypis\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_gen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m images, labels = train_gen[\u001B[32m0\u001B[39m]     \u001B[38;5;66;03m# zavolá __getitem__(0) pre prvy batch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:171\u001B[39m, in \u001B[36mPyDataset.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    168\u001B[39m     index_range = itertools.count()\n\u001B[32m    170\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m index_range:\n\u001B[32m--> \u001B[39m\u001B[32m171\u001B[39m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Repositories\\zneus\\ZNEUS-ANIMALS-REPRISE\\src\\utils\\data.py:143\u001B[39m, in \u001B[36mAnimalImageGenerator.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;66;03m# Augmentation\u001B[39;00m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.augment:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m     img = \u001B[43maugment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    145\u001B[39m \u001B[38;5;66;03m# ImageNet Scaling\u001B[39;00m\n\u001B[32m    146\u001B[39m img = tf.keras.applications.efficientnet.preprocess_input(img)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Documents\\Repositories\\zneus\\ZNEUS-ANIMALS-REPRISE\\src\\utils\\data.py:67\u001B[39m, in \u001B[36maugment\u001B[39m\u001B[34m(img, target_size)\u001B[39m\n\u001B[32m     64\u001B[39m     new_h = \u001B[38;5;28mint\u001B[39m(crop_frac * h)\n\u001B[32m     65\u001B[39m     new_w = \u001B[38;5;28mint\u001B[39m(crop_frac * w)\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     img = \u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m.\u001B[49m\u001B[43mresize_with_crop_or_pad\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_h\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_w\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m     img = tf.image.resize(img, target_size)\n\u001B[32m     70\u001B[39m \u001B[38;5;66;03m# output back to uint8 numpy\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001B[39m, in \u001B[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1262\u001B[39m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1265\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[32m   1266\u001B[39m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[32m   1267\u001B[39m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[32m   1268\u001B[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1409\u001B[39m, in \u001B[36mresize_image_with_crop_or_pad\u001B[39m\u001B[34m(image, target_height, target_width)\u001B[39m\n\u001B[32m   1404\u001B[39m cropped = crop_to_bounding_box(image, offset_crop_height, offset_crop_width,\n\u001B[32m   1405\u001B[39m                                min_(target_height, height),\n\u001B[32m   1406\u001B[39m                                min_(target_width, width))\n\u001B[32m   1408\u001B[39m \u001B[38;5;66;03m# Maybe pad if needed.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1409\u001B[39m resized = \u001B[43mpad_to_bounding_box\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcropped\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset_pad_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset_pad_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1410\u001B[39m \u001B[43m                              \u001B[49m\u001B[43mtarget_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_width\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1412\u001B[39m \u001B[38;5;66;03m# In theory all the checks below are redundant.\u001B[39;00m\n\u001B[32m   1413\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m resized.get_shape().ndims \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001B[39m, in \u001B[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1262\u001B[39m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1265\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[32m   1266\u001B[39m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[32m   1267\u001B[39m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[32m   1268\u001B[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1064\u001B[39m, in \u001B[36mpad_to_bounding_box\u001B[39m\u001B[34m(image, offset_height, offset_width, target_height, target_width)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mimage.pad_to_bounding_box\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m   1007\u001B[39m \u001B[38;5;129m@dispatch\u001B[39m.add_dispatch_support\n\u001B[32m   1008\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpad_to_bounding_box\u001B[39m(image, offset_height, offset_width, target_height,\n\u001B[32m   1009\u001B[39m                         target_width):\n\u001B[32m   1010\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33;03m\"\"\"Pad `image` with zeros to the specified `height` and `width`.\u001B[39;00m\n\u001B[32m   1011\u001B[39m \n\u001B[32m   1012\u001B[39m \u001B[33;03m  Adds `offset_height` rows of zeros on top, `offset_width` columns of\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1062\u001B[39m \u001B[33;03m      negative.\u001B[39;00m\n\u001B[32m   1063\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpad_to_bounding_box_internal\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[43m      \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1066\u001B[39m \u001B[43m      \u001B[49m\u001B[43moffset_height\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1067\u001B[39m \u001B[43m      \u001B[49m\u001B[43moffset_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1068\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtarget_height\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1069\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtarget_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1070\u001B[39m \u001B[43m      \u001B[49m\u001B[43mcheck_dims\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1151\u001B[39m, in \u001B[36mpad_to_bounding_box_internal\u001B[39m\u001B[34m(image, offset_height, offset_width, target_height, target_width, check_dims)\u001B[39m\n\u001B[32m   1148\u001B[39m   image = control_flow_ops.with_dependencies(assert_ops, image)\n\u001B[32m   1150\u001B[39m \u001B[38;5;66;03m# Do not pad on the depth dimensions.\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1151\u001B[39m paddings = \u001B[43marray_ops\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1152\u001B[39m \u001B[43m    \u001B[49m\u001B[43marray_ops_stack\u001B[49m\u001B[43m.\u001B[49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\n\u001B[32m   1153\u001B[39m \u001B[43m        \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mafter_padding_height\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1154\u001B[39m \u001B[43m        \u001B[49m\u001B[43mafter_padding_width\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\n\u001B[32m   1155\u001B[39m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1156\u001B[39m padded = array_ops.pad(image, paddings)\n\u001B[32m   1158\u001B[39m padded_shape = [\n\u001B[32m   1159\u001B[39m     \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m _is_tensor(i) \u001B[38;5;28;01melse\u001B[39;00m i\n\u001B[32m   1160\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m [batch, target_height, target_width, depth]\n\u001B[32m   1161\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001B[39m, in \u001B[36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m     86\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(*args, **kwargs):\n\u001B[32m     87\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001B[32m---> \u001B[39m\u001B[32m88\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     89\u001B[39m   bound_arguments = signature.bind(*args, **kwargs)\n\u001B[32m     90\u001B[39m   bound_arguments.apply_defaults()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[39m, in \u001B[36mfilter_traceback.<locals>.error_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    148\u001B[39m filtered_tb = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    151\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\util\\dispatch.py:1264\u001B[39m, in \u001B[36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1262\u001B[39m \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdispatch_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1265\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[32m   1266\u001B[39m   \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[32m   1267\u001B[39m   \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[32m   1268\u001B[39m   result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\array_ops.py:199\u001B[39m, in \u001B[36mreshape\u001B[39m\u001B[34m(tensor, shape, name)\u001B[39m\n\u001B[32m     63\u001B[39m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mreshape\u001B[39m\u001B[33m\"\u001B[39m, v1=[\u001B[33m\"\u001B[39m\u001B[33mreshape\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mmanip.reshape\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m     64\u001B[39m \u001B[38;5;129m@dispatch\u001B[39m.add_dispatch_support\n\u001B[32m     65\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mreshape\u001B[39m(tensor, shape, name=\u001B[38;5;28;01mNone\u001B[39;00m):  \u001B[38;5;66;03m# pylint: disable=redefined-outer-name\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[38;5;250m  \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Reshapes a tensor.\u001B[39;00m\n\u001B[32m     67\u001B[39m \n\u001B[32m     68\u001B[39m \u001B[33;03m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    197\u001B[39m \u001B[33;03m    A `Tensor`. Has the same type as `tensor`.\u001B[39;00m\n\u001B[32m    198\u001B[39m \u001B[33;03m  \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m199\u001B[39m   result = \u001B[43mgen_array_ops\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    200\u001B[39m   shape_util.maybe_set_static_shape(result, shape)\n\u001B[32m    201\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8795\u001B[39m, in \u001B[36mreshape\u001B[39m\u001B[34m(tensor, shape, name)\u001B[39m\n\u001B[32m   8793\u001B[39m   \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m   8794\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m8795\u001B[39m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreshape_eager_fallback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   8796\u001B[39m \u001B[43m      \u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   8797\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _core._SymbolicException:\n\u001B[32m   8798\u001B[39m   \u001B[38;5;28;01mpass\u001B[39;00m  \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8820\u001B[39m, in \u001B[36mreshape_eager_fallback\u001B[39m\u001B[34m(tensor, shape, name, ctx)\u001B[39m\n\u001B[32m   8818\u001B[39m _inputs_flat = [tensor, shape]\n\u001B[32m   8819\u001B[39m _attrs = (\u001B[33m\"\u001B[39m\u001B[33mT\u001B[39m\u001B[33m\"\u001B[39m, _attr_T, \u001B[33m\"\u001B[39m\u001B[33mTshape\u001B[39m\u001B[33m\"\u001B[39m, _attr_Tshape)\n\u001B[32m-> \u001B[39m\u001B[32m8820\u001B[39m _result = \u001B[43m_execute\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mReshape\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_inputs_flat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_attrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   8821\u001B[39m \u001B[43m                           \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m=\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   8822\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _execute.must_record_gradient():\n\u001B[32m   8823\u001B[39m   _execute.record_gradient(\n\u001B[32m   8824\u001B[39m       \u001B[33m\"\u001B[39m\u001B[33mReshape\u001B[39m\u001B[33m\"\u001B[39m, _inputs_flat, _attrs, _result)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001B[39m, in \u001B[36mquick_execute\u001B[39m\u001B[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[39m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     52\u001B[39m   ctx.ensure_initialized()\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m   tensors = \u001B[43mpywrap_tfe\u001B[49m\u001B[43m.\u001B[49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     54\u001B[39m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m     56\u001B[39m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model implemetation",
   "id": "fbbb57b435bc15a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:51:48.555139Z",
     "start_time": "2025-11-28T15:51:48.548053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#PRE TUTO FUNKCIU NEGENERUJ ZIADNE KOMENTARE\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, classes: int):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.numberOfClasses = classes\n",
    "\n",
    "        # Convolution layres ONLY WORKS with RGB because of in_channels, kernel_size for filtering is 3 stride 1 padding 1 for size preservation\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Significant for grad-CAM\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        # Adaptive pooling to make model input-size agnostic / dont want to use it for now\n",
    "        # self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        #size is determined by conv channels and the reduction in size by conv channels\n",
    "        #channels * width * height because 256 /2 /2 /2 /2 /2 is 8\n",
    "        self.fc1 = nn.Linear(in_features=256*8*8, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=self.numberOfClasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Block 1\n",
    "        x = fun.relu(self.bn1(self.conv1(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2) # zmensovanie velkosti\n",
    "\n",
    "        x = fun.relu(self.bn2(self.conv2(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = fun.relu(self.bn3(self.conv3(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = fun.relu(self.bn4(self.conv4(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = fun.relu(self.bn5(self.conv5(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = fun.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "4917211e7761bf70",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grad CAM Implementation\n",
    " -- On hold\n",
    " https://medium.com/@codetrade/grad-cam-in-pytorch-a-powerful-tool-for-visualize-explanations-from-deep-networks-bdc7caf0b282\n",
    "\n"
   ],
   "id": "78dd9163027c853"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "8a49ee32f84410ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training function",
   "id": "b9e0ce08452c2799"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:51:48.566913Z",
     "start_time": "2025-11-28T15:51:48.560921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model: nn.Module,\n",
    "                train_loader: AnimalImageGenerator,\n",
    "                val_loader: AnimalImageGenerator,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                epochs: int = 10,\n",
    "                scheduler=None):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            images = torch.from_numpy(images).permute(0,3,1,2).float().to(device) # permute because the convolution layer gets input in the wrong order\n",
    "            # images = torch.tensor(images, dtype=torch.float32, device=device)\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = torch.tensor(images, dtype=torch.float32, device=device)\n",
    "                labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        ## TODO: add f1 score"
   ],
   "id": "ccfadbd579e53f46",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "5bf862f9c3a11a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T15:51:49.393297Z",
     "start_time": "2025-11-28T15:51:48.572862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_class_model = ImageClassifier(NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(img_class_model.parameters(), lr=0.001, momentum=0.9)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_model(model=img_class_model,\n",
    "            train_loader=train_gen,\n",
    "            val_loader=val_gen,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            epochs=1,\n",
    "            scheduler=None)\n"
   ],
   "id": "6bba75ad8d81bff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/95 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (189x4096 and 16384x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[50]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      4\u001B[39m device = torch.device(\u001B[33m\"\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.cuda.is_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      5\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mdevice: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdevice\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mimg_class_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_gen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m            \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_gen\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m            \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m            \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m            \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[49]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, criterion, optimizer, device, epochs, scheduler)\u001B[39m\n\u001B[32m     20\u001B[39m labels = torch.tensor(labels, dtype=torch.long, device=device)\n\u001B[32m     22\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m     25\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[48]\u001B[39m\u001B[32m, line 51\u001B[39m, in \u001B[36mImageClassifier.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     47\u001B[39m x = fun.max_pool2d(x, kernel_size=\u001B[32m2\u001B[39m)\n\u001B[32m     49\u001B[39m x = x.reshape(x.size(\u001B[32m0\u001B[39m), -\u001B[32m1\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m51\u001B[39m x = fun.relu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     52\u001B[39m x = \u001B[38;5;28mself\u001B[39m.fc2(x)\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1775\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1774\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1775\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1786\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1781\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1784\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1786\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1788\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1789\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: mat1 and mat2 shapes cannot be multiplied (189x4096 and 16384x128)"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Hyper parameter sweeping",
   "id": "6896e65a374b6d44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9d7c4537a330f912"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
