{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Machine Learning - Image Classification",
   "id": "de369611418338b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:52:46.452793Z",
     "start_time": "2025-11-28T23:52:40.827889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#importing\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fun\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import *"
   ],
   "id": "3d39034e8143cb63",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Database importing",
   "id": "36370301af86524b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.130001Z",
     "start_time": "2025-11-28T23:52:46.456776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "DAT_PATH = os.getenv(\"TRAIN_DATASET_PATH\")\n",
    "\n",
    "ANIMALS_DATAFRAME = load_dataset_info(\"../data/archive/raw-img\")"
   ],
   "id": "d9710a1ff3f0f4bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.196314Z",
     "start_time": "2025-11-28T23:53:00.187453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_df, test_df = train_test_split(ANIMALS_DATAFRAME, test_size=0.01, random_state=37)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.0526, random_state=37)"
   ],
   "id": "5cc1f9d00df31223",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.208668Z",
     "start_time": "2025-11-28T23:53:00.201724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TARGET_SIZE = (256, 256)\n",
    "MAX_SIZE = 500\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = ANIMALS_DATAFRAME.label.nunique()\n",
    "CLASS_LABELS = {name: idx for idx, name in enumerate(np.sort(ANIMALS_DATAFRAME.label.unique()))}\n",
    "\n",
    "# vypocet frekvencie augmentacie -> aby nebol model biasnuty iba na majoritne categorie obrazkov\n",
    "counts = train_df.label.value_counts()\n",
    "max_count = counts.max()\n",
    "aug_strength = (max_count / counts).to_dict()\n",
    "\n",
    "print(aug_strength)\n"
   ],
   "id": "abb2166b0f0fe21b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cane': 1.0, 'ragno': 1.0813374436803416, 'gallina': 1.5670103092783505, 'cavallo': 1.8431689571544059, 'mucca': 2.608695652173913, 'scoiattolo': 2.632794457274827, 'farfalla': 2.9552819183408943, 'pecora': 3.375277572168764, 'gatto': 3.965217391304348, 'elefante': 4.470588235294118}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.216295Z",
     "start_time": "2025-11-28T23:53:00.213947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#setting up dataloaders#\n",
    "#   POMALE\n",
    "#\n",
    "# train_gen = AnimalImageGenerator(\n",
    "#     df=train_df,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     target_size=TARGET_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     augment=True,\n",
    "#     shuffle=True,\n",
    "#     aug_strength=aug_strength,\n",
    "#     max_size=MAX_SIZE,\n",
    "#     class_mapping=CLASS_LABELS,\n",
    "# )\n",
    "#\n",
    "# test_gen = AnimalImageGenerator(\n",
    "#     df=test_df,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     target_size=TARGET_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     augment=False,\n",
    "#     shuffle=False,\n",
    "#     max_size=MAX_SIZE,\n",
    "#     class_mapping=CLASS_LABELS,\n",
    "# )\n",
    "#\n",
    "# val_gen = AnimalImageGenerator(\n",
    "#     df=val_df,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     target_size=TARGET_SIZE,\n",
    "#     num_classes=NUM_CLASSES,\n",
    "#     augment=False,\n",
    "#     shuffle=False,\n",
    "#     max_size=MAX_SIZE,\n",
    "#     class_mapping=CLASS_LABELS,\n",
    "# )\n"
   ],
   "id": "6a18fe6f83c8e0b6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:20.927534Z",
     "start_time": "2025-11-28T23:53:20.920518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "\n",
    "#CHATOVINA ale ze vraj rychla\n",
    "\n",
    "def create_sampler(df, class_mapping):\n",
    "    labels = df[\"label\"].map(class_mapping).values\n",
    "    class_counts = np.bincount(labels)\n",
    "    class_weights = 1.0 / class_counts\n",
    "\n",
    "    sample_weights = class_weights[labels]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        torch.from_numpy(sample_weights).float(),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    return sampler\n",
    "\n",
    "train_dataset = AnimalDataset(\n",
    "    df=train_df,\n",
    "    class_mapping=CLASS_LABELS,\n",
    "    augment=True,\n",
    "    target_size=TARGET_SIZE,\n",
    ")\n",
    "\n",
    "val_dataset = AnimalDataset(\n",
    "    df=val_df,\n",
    "    class_mapping=CLASS_LABELS,\n",
    "    augment=False,\n",
    "    target_size=TARGET_SIZE,\n",
    ")\n",
    "\n",
    "sampler = create_sampler(train_df, CLASS_LABELS)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n"
   ],
   "id": "22dcbd10e815ce22",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model implemetation",
   "id": "fbbb57b435bc15a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:23.757896Z",
     "start_time": "2025-11-28T23:53:23.754343Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#PRE TUTO FUNKCIU NEGENERUJ ZIADNE KOMENTARE\n",
    "class ImageClassifier(nn.Module):\n",
    "    def __init__(self, classes: int):\n",
    "        super(ImageClassifier, self).__init__()\n",
    "        self.numberOfClasses = classes\n",
    "\n",
    "        # Convolution layres ONLY WORKS with RGB because of in_channels, kernel_size for filtering is 3 stride 1 padding 1 for size preservation\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Significant for grad-CAM\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Adaptive pooling to make model input-size agnostic / dont want to use it for now\n",
    "        # self.adaptive_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "\n",
    "        #size is determined by conv channels and the reduction in size by conv channels\n",
    "        #channels * width * height because 256 /2 /2 /2 is 8\n",
    "        self.fc1 = nn.Linear(in_features=64*32*32, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=self.numberOfClasses)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Block 1\n",
    "        x = fun.relu(self.bn1(self.conv1(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2) # zmensovanie velkosti\n",
    "\n",
    "        x = fun.relu(self.bn2(self.conv2(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = fun.relu(self.bn3(self.conv3(x)))\n",
    "        x = fun.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "\n",
    "        x = fun.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ],
   "id": "6106294ca60c4799",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Training",
   "id": "83f8c73d348a535d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training function",
   "id": "ffeeeb5c4063c3eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:59.442025Z",
     "start_time": "2025-11-28T23:53:59.437651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(model: nn.Module,\n",
    "                train_loader: AnimalImageGenerator,\n",
    "                val_loader: AnimalImageGenerator,\n",
    "                criterion: nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                epochs: int = 10,\n",
    "                scheduler=None):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # for epoch in tqdm(range(1, epochs+1), desc=\"Training model\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # for images, labels in train_loader:\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.float().to(device)\n",
    "            labels = labels.long().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.float().to(device)\n",
    "                labels = labels.long().to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        ## TODO: add f1 score"
   ],
   "id": "e17813431b74b029",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "4d4668dc88418f3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set-up",
   "id": "d4d05874f7518aae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:54:01.102748Z",
     "start_time": "2025-11-28T23:54:01.052241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_class_model = ImageClassifier(NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(img_class_model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n"
   ],
   "id": "d4f85be2913fa843",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:57:54.825620Z",
     "start_time": "2025-11-28T23:54:01.967698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRAINING\n",
    "train_model(model=img_class_model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            epochs=5,\n",
    "            scheduler=None)"
   ],
   "id": "e5df0fcd7b09431b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [00:47<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 1.9299, Train Acc: 0.3129 | Val Loss: 1.8250, Val Acc: 0.3751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [00:42<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Train Loss: 1.7721, Train Acc: 0.3726 | Val Loss: 1.6132, Val Acc: 0.4330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [00:41<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Train Loss: 1.7043, Train Acc: 0.3996 | Val Loss: 1.5206, Val Acc: 0.4655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [00:42<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Train Loss: 1.6668, Train Acc: 0.4100 | Val Loss: 1.4757, Val Acc: 0.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355/355 [00:42<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Train Loss: 1.6232, Train Acc: 0.4290 | Val Loss: 1.4039, Val Acc: 0.5083\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Grad CAM Implementation\n",
    " -- On hold\n",
    " https://medium.com/@codetrade/grad-cam-in-pytorch-a-powerful-tool-for-visualize-explanations-from-deep-networks-bdc7caf0b282"
   ],
   "id": "b86d2775061e13df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Single image predictions",
   "id": "5a88dc79ad1b72a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.376608Z",
     "start_time": "2025-11-28T23:46:53.168390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SingleImageInput():\n",
    "    def __init__(self, target_size=(256, 256)):\n",
    "        self.target_size = target_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ])\n",
    "    def read_image(self, path) -> torch.Tensor:\n",
    "        print(path)\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transform(img)\n",
    "        return img"
   ],
   "id": "ef644116c2c8e139",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.376608Z",
     "start_time": "2025-11-28T23:51:41.640205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ran_img_sample = pick_random(ANIMALS_DATAFRAME, ANIMALS_DATAFRAME.label == \"cane\")\n",
    "print(ran_img_sample)\n",
    "ImageReader = SingleImageInput(target_size=(256, 256))\n",
    "img = ImageReader.read_image(ran_img_sample).to(device)\n",
    "img.unsqueeze_(0)\n",
    "\n",
    "img_class_model.eval()\n",
    "out = img_class_model(img)\n",
    "predicted_l = out.argmax(dim=1).item()"
   ],
   "id": "75921675d9177435",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\archive\\raw-img\\cane\\OIP--MffROkglTy0W_sVV_zv3AHaFY.jpeg\n",
      "..\\data\\archive\\raw-img\\cane\\OIP--MffROkglTy0W_sVV_zv3AHaFY.jpeg\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T23:53:00.376608Z",
     "start_time": "2025-11-28T23:51:43.281551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_layer = img_class_model.conv2 ## last layer\n",
    "activations = []\n",
    "gradients = []\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations.append(output)\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    gradients.append(grad_output[0])\n",
    "\n",
    "f_handle = target_layer.register_forward_hook(forward_hook)\n",
    "b_handle = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "try:\n",
    "    img_class_model.zero_grad()\n",
    "    score = out[0, predicted_l]\n",
    "    score.backward()\n",
    "finally:\n",
    "    f_handle.remove()\n",
    "    b_handle.remove()\n",
    "\n",
    "print(len(gradients))\n",
    "\n",
    "weights = torch.mean(gradients[0], dim=[2, 3])\n",
    "\n",
    "heatmap = torch.sum(weights * activations[0], dim=1).squeeze()\n",
    "heatmap = np.maximum(heatmap.cpu().detach().numpy(), 0)\n",
    "heatmap /= np.max(heatmap)  # normalization\n",
    "\n",
    "weights = torch.mean(gradients[0], dim=[2, 3])\n",
    "\n",
    "heatmap = torch.sum(weights * activations[0], dim=1).squeeze()\n",
    "heatmap = np.maximum(heatmap.cpu().detach().numpy(), 0)\n",
    "heatmap /= np.max(heatmap)  # normalization\n",
    "\n",
    "heatmap = cv2.resize(heatmap, (ran_img_sample.size[1], ran_img_sample.size[0]))\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "superimposed_img = cv2.addWeighted(ran_img_sample, 0.6, heatmap, 0.4, 0)\n",
    "\n",
    "cv2.imshow('Grad-CAM', superimposed_img)"
   ],
   "id": "155d7bb6e33607d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     19\u001B[39m     b_handle.remove()\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(gradients))\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m weights = torch.mean(\u001B[43mgradients\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m, dim=[\u001B[32m2\u001B[39m, \u001B[32m3\u001B[39m])\n\u001B[32m     25\u001B[39m heatmap = torch.sum(weights * activations[\u001B[32m0\u001B[39m], dim=\u001B[32m1\u001B[39m).squeeze()\n\u001B[32m     26\u001B[39m heatmap = np.maximum(heatmap.cpu().detach().numpy(), \u001B[32m0\u001B[39m)\n",
      "\u001B[31mIndexError\u001B[39m: list index out of range"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "610af3b975dcc7cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
